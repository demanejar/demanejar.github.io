[ { "title": "Tìm hiểu tổng quan k8s", "url": "/posts/k8s/", "categories": "Blogging", "tags": "k8s", "date": "2025-04-12 20:52:00 +0700", "snippet": "Một số tìm hiểu ban đầu về k8s. Mối liên hệ giữa containerd và docker Low level run time hiện tại chủ yếu là runc High level runtime hiện tại chủ yếu là dùng containerdgraph TD; A[Docker CLI]--&amp;gt;|docker run ...|B[Docker Engine]; B[Docker Daemon]; B --&amp;gt;C[Containerd]; C ..." }, { "title": "Cài đặt Selenium Middleware cho Scrapy", "url": "/posts/selenium-middleware-custom-scrapy/", "categories": "Crawler", "tags": "Crawler, Selenium, Scrapy", "date": "2024-12-03 20:52:00 +0700", "snippet": "Scrapy có cung cấp thư viện scrapy-selenium cho phép việc sử dụng Seleinum để lấy dữ liệu trang web trước khi trả về cho spdier xử lý. Tuy nhiên trong 1 số trường hợp ví dụ mình không muốn dùng selenium bình thường mà mình muốn dùng undetected-chromedriver vì nó giúp bypass Cloudflare trên websit..." }, { "title": "Một số thủ thuật nhỏ khi crawl data", "url": "/posts/some-trick-crawler/", "categories": "Crawler", "tags": "Crawler", "date": "2024-10-01 20:52:00 +0700", "snippet": "Biến của Javascript có thể chứa dữ liệu cần thiết khi crawl website render bằng JavascriptKhi crawl các trang web render bằng Javascript như bài viết trước chúng ta phải sử dụng một trình biên dịch phía trước để dịch các đoạn mã Javascript này để chúng render ra HTML trước khi tiến hành phân tích..." }, { "title": "Spline | Data Lineage Tracking And Visualization Solution", "url": "/posts/data-lineage-tracking-and-visualization-solution-with-spline/", "categories": "Blogging", "tags": "Data Lineage, Spark, Spline, Spark Agent", "date": "2024-03-14 20:52:00 +0700", "snippet": "Spline là một công cụ OpenSource cho phép tự động theo dõi Data Lineage và Data Pipeline Structure. Công việc phổ biến nhất của nó là theo dõi và trực quan hóa Data Lineage cho Spark.Tổng quan SplineSpline là một công cụ mã nguồn mở và miễn phí để theo dõi tự động dòng dữ liệu (data lineage) và c..." }, { "title": "Hướng dẫn Airflow HA", "url": "/posts/airflow-ha/", "categories": "Blogging", "tags": "HA, Airflow", "date": "2024-02-18 20:52:00 +0700", "snippet": "Trong bài này mình sẽ hướng dẫn cài đặt Airflow và cài HA cho nó. môi trường sử dụng là máy ảo virtualbox tạo 2 máy ảo với địa chỉ cố định, add user và cập quyền ssh từ host VAGRANT_COMMAND = ARGV[0]Vagrant.configure(&quot;2&quot;) do |config| if VAGRANT_COMMAND == &quot;ssh&quot; confi..." }, { "title": "Crawl báo song ngữ với Scrapy và Splash", "url": "/posts/scrapy-with-splash/", "categories": "Crawler", "tags": "Crawler, Scrapy, Splash", "date": "2023-04-15 20:52:00 +0700", "snippet": "Tìm hiểu thêm về khái niệm, ưu nhược điểm của Client Side và Server Side Rendering qua bài viết Client-Side và Server-Side Rendering.Chúng ta sẽ thấy có một số website client side rendering, các đoạn mã HTML của nó sẽ được gen ra ở phía trình duyệt người dùng, vì thế khi crawl mặc dù F12 thấy đầy..." }, { "title": "Giới thiệu Scrapy Shell", "url": "/posts/scrapy-shell/", "categories": "Crawler", "tags": "Crawler, Scrapy, Scrapy Shell", "date": "2023-04-01 20:52:00 +0700", "snippet": "Mỗi lần viết 1 spider chúng ta phải viết nhiều các đoạn css selector, xpath để phân tích thông tin mà nhiều lúc không biết nó đúng hay sai. Mỗi lần như vậy thì lại phải chạy project rồi in ra thông tin mình crawl được xem có đúng hay không. Làm như vậy thì rất mất thời gian, vì thế Scrapy có cung..." }, { "title": "PHP Scraper", "url": "/posts/php-scraper/", "categories": "Crawler", "tags": "Crawler, PHP, PHP crawler", "date": "2023-03-15 20:52:00 +0700", "snippet": "Nói về crawl chắc hẳn mọi thứ đều đổ dồn về Python và các framework xây dựng trên Python như Scrapy, Beautiful Soup hay Selenium sử dụng Python,… Trong bài viết này ngoại truyện một tí, chúng ta sẽ nói về một ngôn ngữ mà không được mạnh lắm về mảng này, PHP.Sẽ có rất nhiều lúc mà Python sẽ không ..." }, { "title": "Crawl 1000 trang báo với Scrapy và MySQL", "url": "/posts/crawl-1000-website-new-with-scrapy/", "categories": "Crawler", "tags": "Crawler, Scrapy, alonhadat, MySQL", "date": "2023-03-01 20:52:00 +0700", "snippet": "Nếu với mỗi website lại viết 1 spider để phân tích thông tin thì sẽ rất mất thời gian, nhất là với các website tin tức, có hàng ngàn các website tin tức khác nhau và chúng còn mọc ra mỗi ngày.Vậy bây giờ có một bài toán đặt ra là cần phân tích nội dung của 1000 website báo chí, và nhiệm vụ của ch..." }, { "title": "Cấu hình proxy cho project Scrapy", "url": "/posts/add-proxy-to-scrapy-project/", "categories": "Crawler", "tags": "Crawler, Scrapy, alonhadat, proxy", "date": "2023-02-15 20:52:00 +0700", "snippet": "Proxy chắc là khái niệm đã không còn xa lạ gì với tất cả mọi người. Với người làm về crawl dữ liệu thì proxy như vật bất ly thân. Trong bài viết này, mình sẽ hướng dẫn cách cấu hình proxy cho project Scrapy. Project mình sử dụng để làm ví dụ cho bài viết này là project crawl alonhadat https://git..." }, { "title": "Crawl dữ liệu nhà đất từ alonhadat với Scrapy", "url": "/posts/crawl-housing-data-from-alonhadat/", "categories": "Crawler", "tags": "Crawler, Scrapy, alonhadat", "date": "2023-01-24 20:52:00 +0700", "snippet": "Trong bài viết này mình sẽ giới thiệu chi tiết về cách tạo một project với Scrapy và sử dụng để phân tích lấy dữ liệu nhà đất từ trang alonhadat. Nếu máy bạn chưa có Scrapy thì có thể cài đặt bằng pip, xem chi tiết tại website https://pypi.org/project/Scrapy/. Nếu bạn quan tâm tới một luồng dữ ..." }, { "title": "Tại sao những người làm website không hoàn toàn muốn bảo vệ website của họ khỏi bị crawl?", "url": "/posts/no-need-protected-website-from-scraping/", "categories": "Crawler", "tags": "Crawler, Scrapy, Selenium, Protected Website", "date": "2023-01-24 20:52:00 +0700", "snippet": "Các website hiện nay thì đã không còn dễ lấy dữ liệu như ngày trước nữa vì cấu trúc của các website bây giờ cũng khác xưa rất là nhiều, nó không có các phần được định nghĩa rõ ràng để phân tích nhanh chóng, và một phần lớn khác là vì một số website vừa và lớn cũng đã áp dụng một số các biện pháp..." }, { "title": "Crawler, một số điều mình chia sẻ về crawler và loạt bài viết về crawler sắp tới?", "url": "/posts/what-is-crawler-and-something/", "categories": "Crawler", "tags": "Crawler, Scrapy, Selenium", "date": "2023-01-12 20:52:00 +0700", "snippet": "Crawler, Web Scrape, Web Scraping, thu thập dữ liệu, cào dữ liệu,… chắc là các từ ngữ mà chúng ta hay sử dụng nhất để nói về các công việc tạo ra những chương trình đi phân tích và lấy dữ liệu từ một website. Chắc chắn là có sự khác biệt giữa hai khái niệm web crawler và web scraping, tuy nhiên v..." }, { "title": "Bài toán phân cụm với Spark ML và xây dựng ứng dụng với Flask", "url": "/posts/sparkml/", "categories": "Hadoop & Spark, Spark", "tags": "Bigdata, Spark, Flask, Spark MLlib, LDA", "date": "2021-11-15 20:52:00 +0700", "snippet": "Spark MLlib là thư viện học máy của Spark được tạo ra với mục tiêu có thể giải quyết các bài toán ML một cách dễ dàng hơn. Mặc dùng các thư viện mà MLlib mang lại không phong phú bằng python với những sklearn, tensorflow, PyTorch,… Tuy nhiên MLlib hỗ trợ cho chương trình của bạn có thể chạy phân ..." }, { "title": "Giải thích về các chế độ chính khi chạy Spark", "url": "/posts/mode-in-spark/", "categories": "Hadoop & Spark, Spark", "tags": "Bigdata, Spark, Yarn, Mesos, Apache Mesos", "date": "2021-11-02 20:52:00 +0700", "snippet": "Khi sử dụng Spark các bạn có thể thấy có rất là nhiều các chế độ khác nhau như local, standalone, yarn,… chắc hẳn rất nhiều người còn chưa hiểu rõ về các chế độ này nhất là khi mình sử dụng các chế độ trong từng bài viết của mình trên blog, trong bài viết này mình sẽ nói rõ về các chế độ và tại ..." }, { "title": "Spark Streaming với Kafka", "url": "/posts/spark-streaming-kafka/", "categories": "Hadoop & Spark, Spark", "tags": "Bigdata, Spark, Kafka, Kafka Consumer", "date": "2021-10-14 20:52:00 +0700", "snippet": "Trong 2 bài ví dụ về Spark Streaming trước thì mình đã minh họa về Spark Streaming nhận dữ liệu qua socket và xử lý chúng. Tuy nhiên, trong thực tế thì ít khi chúng ta sử dung socket để truyền và xử lý dữ liệu thay vào đó chúng ta sẽ thường sử dụng các hàng đợi tin nhắn (Message Queue) mà tiêu bi..." }, { "title": "Cài đặt Zeppelin Notebook", "url": "/posts/install-zeppelin/", "categories": "Hadoop & Spark, Spark", "tags": "Bigdata, Spark, Zeppelin", "date": "2021-10-01 20:52:00 +0700", "snippet": "Chắc chúng ta quen nhiều hơn với Jupyter notebook và Zeppelin notebook có thể còn chưa được nghe tới bao giờ. Zeppelin notebook hay Apache Zeppelin là một ứng dụng dựa trên web cho phép phân tương tác trực tiếp với SQL, Scala, Python, R và hơn thế nữa.Tải về ZeppelinTrước hết bạn tải về file cài ..." }, { "title": "Project Log Analyzer với Spark Streaming", "url": "/posts/log-analyzer/", "categories": "Hadoop & Spark, Spark", "tags": "Spark Streaming, Bigdata, Spark", "date": "2021-09-28 20:52:00 +0700", "snippet": "Bài viết trước chúng ta đã làm quen với Spark Streaming với một project đơn giản về lọc từ, trong bài viết này chúng ta sẽ xem xét project phức tạp hơn một tí về phân tích log.Chuẩn bị ProjectToàn bộ project các bạn có thể xem tại: https://github.com/demanejar/logs-analyzer, mọi người clone proje..." }, { "title": "Project Socket Stream với Spark Streaming", "url": "/posts/socket-stream/", "categories": "Hadoop & Spark, Spark", "tags": "Spark Streaming, Bigdata, Spark", "date": "2021-09-23 20:52:00 +0700", "snippet": "Trong bài viết này chúng ta sẽ đi xét một ví dụ nhỏ với Spark Streaming. Công việc của chúng ta là tạo một project với Spark Streaming lắng nghe ở cổng 7777 và lọc những dòng có chứa từ “error” rồi in chúng ra.Chuẩn bị ProjectProject rất đơn giản, được viết bằng Scala. Bạn có thể xem toàn bộ proj..." }, { "title": "Đa luồng và đa tiến trình trong Python", "url": "/posts/Multithread-Multiprocess-in-Python/", "categories": "Blogging", "tags": "Python", "date": "2021-09-20 14:52:00 +0700", "snippet": "Trong một lần phỏng vấn mình có được hỏi về các khái niệm này, lúc đó do kiến thức mình hiểu có phần bị sai vì thế mình quyết định là tìm hiểu lại và viết lại một số vấn đề trong lập trình đa luồng và đa tiến trình trong Python1. Một số khái niệm cơ bảnTiến trình (Process)Tiến trình đơn giản có t..." }, { "title": "Spark Streaming", "url": "/posts/spark-streaming/", "categories": "Hadoop & Spark, Spark", "tags": "Spark Streaming, Bigdata, Spark", "date": "2021-09-16 20:52:00 +0700", "snippet": "Tổng quanSpark Streaming là một bộ mở rộng của core Spark API cho phép mở rộng, thông lượng cao, có khả năng chịu lỗi. Spark Streaming được thiết kế để xử lý dữ liệu dạng streams.Dữ liệu đầu vào từ Spark có thể lấy từ Kafka, Flume, Kinesis hoặc TCP sockets là các dữ liệu động được gửi vào liên tụ..." }, { "title": "Window function, pivot trong Spark SQL (Part 2)", "url": "/posts/spark-sql-window-function-pivot-part-2/", "categories": "Hadoop & Spark, Spark", "tags": "Spark SQL, Bigdata, Spark, pivot spark, window function", "date": "2021-09-14 08:52:00 +0700", "snippet": "Nếu bạn chưa xem phần 1 thì có thể xem lại TẠI ĐÂY nha, bài viết hôm nay mình sẽ giới thiệu tiếp tới mọi người một số ví dụ về window function và pivot sâu hơn để mọi người có thể hiểu rõ hơn về window function và pivot trong SparkExample 5: Window functionYêu cầuCho tập dữ liệu dạng .csv như sau..." }, { "title": "Window function, pivot trong Spark SQL", "url": "/posts/spark-sql-window-function-pivot/", "categories": "Hadoop & Spark, Spark", "tags": "Spark SQL, Bigdata, Spark, pivot spark, window function", "date": "2021-09-09 08:52:00 +0700", "snippet": "Window aggregate functions (hay thường được gọi tắt là window functions hoặc windowed aggregates) là hàm giúp hỗ trợ tính toán trên 1 nhóm các bản ghi được gọi là cửa sổ mà có liên quan tới bản ghi hiện tại.Nhìn chung thì những phần này khá trìu tượng về mặt lý thuyết nên đọc lý thuyết càng gây r..." }, { "title": "Phân tích dữ liệu bán lẻ với Spark SQL", "url": "/posts/retail-data-analytics-with-spark-sql/", "categories": "Hadoop & Spark, Spark", "tags": "Spark SQL, Bigdata, Spark", "date": "2021-09-02 20:52:00 +0700", "snippet": "Như đã tìm hiểu ở bài viết trước về Spark SQL, Dataframe và Dataset, Spark SQL là một mô hình để xử lý dữ liệu có cấu trúc của Spark rất phổ biến. Trong bài viết này chúng ta sẽ sử dụng Spark SQL để đi phân tích, xử lý một dữ liệu bán lẻ có cấu trúc cho trướcDữ liệu và chuẩn bị dữ liệuDữ liệuCho ..." }, { "title": "Spark SQL, Dataframe và Dataset", "url": "/posts/spark-sql-dataframe-dataset/", "categories": "Hadoop & Spark, Spark", "tags": "Spark SQL, Bigdata, Spark", "date": "2021-08-26 20:52:00 +0700", "snippet": "Spark SQL là một mô hình để xử lý dữ liệu có cấu trúc của Spark rất phổ biến. Interfaces cung cấp bởi Spark SQL có thêm các thông tin về cấu trúc của dữ liệu và các tính toán đang được thực hiện. Với những thông tin bổ sung này Spark SQL có thể thực hiện các tối ưu hóa bổ sungSQLSpark SQL được th..." }, { "title": "Chương trình Word Count với spark-submit và spark-shell", "url": "/posts/word-count-with-spark-submit-and-spark-shell/", "categories": "Hadoop & Spark, Spark", "tags": "Spark, Apache Spark, Bigdata, Ubuntu, Hadoop MapReduce, HDFS", "date": "2021-08-24 20:52:00 +0700", "snippet": "Wordcount cũng là một chương trình kinh điển khi nhắc tới Spark, một phần cũng là để so sánh hiệu năng với chính Hadoop MapReduce. Trong bài viết này mình sẽ hướng dẫn mọi người tạo và chạy chương trình Wordcount với spark-submit sử dụng Java và spark-shell sử dụng Scala.Khởi động HDFS và SparkVớ..." }, { "title": "Cài đặt Apache Spark standalone", "url": "/posts/install-apache-spark-ubuntu/", "categories": "Hadoop & Spark, Spark", "tags": "Spark, Apache Spark, Bigdata, Ubuntu", "date": "2021-08-19 20:52:00 +0700", "snippet": "Apache Spark là một framework dùng trong xử lý dữ liệu lớn. Nền tảng này trở nên phổ biến rộng rãi do dễ sử dụng và tốc độ xử lý dữ liệu được cải thiện hơn Hadoop. Apache Spark có thể phân phối khối lượng công việc trên một nhóm máy tính trong một cụm để xử lý hiệu quả hơn các tập hợp dữ liệu lớn..." }, { "title": "Spark RDD", "url": "/posts/spark-rdd/", "categories": "Hadoop & Spark, Spark", "tags": "Spark, Apache Spark, Bigdata, RDD", "date": "2021-08-17 20:52:00 +0700", "snippet": "Resilient Distributed Datasets (RDDs)Resilient Distributed Datasets (RDD) là một cấu trúc dữ liệu cơ bản của Spark. Nó là một tập hợp bất biến phân tán của một đối tượng có thể hoạt động song song.Có hai cách để tạo RDDs: Tạo từ một tập hợp dữ liệu có sẵn trong ngôn ngữ sử dụng như Java, Python,..." }, { "title": "Giới thiệu tổng quan về Spark", "url": "/posts/spark-introduction/", "categories": "Hadoop & Spark, Spark", "tags": "Spark, Hadoop, Hadoop vs Spark", "date": "2021-08-12 20:52:00 +0700", "snippet": "Tổng quan về Apache SparkSpark ban đầu được Matei Zaharia bắt đầu tại AMPLab của UC Berkeley vào năm 2009 và được mở nguồn vào năm 2010 theo giấy phép BSD. Vào năm 2013, dự án đã được quyên góp cho Apache Software Foundation và chuyển giấy phép sang Apache 2.0. Vào tháng 2 năm 2014, Spark trở thà..." }, { "title": "Tổng hợp các câu hỏi về Apache Hadoop", "url": "/posts/hadoop-question/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop Yarn", "date": "2021-08-09 20:52:00 +0700", "snippet": "Mục tiêu chính của Apache HadoopLưu trữ dữ liệu khả mở và xử lý dữ liệu mạnh mẽ. Tiết kiệm chi phí khi lưu trữ và xử lý lượng dữ liệu lớn.Bạn có thể xem thêm chi tiết mục tiêu của Hadoop TẠI ĐÂYHadoop giải quyết bài toán chịu lỗi thông qua kỹ thuật gì Hadoop chịu lỗi thông qua kỹ thuật dư thừa ..." }, { "title": "Hadoop MapReduce và chương trình WordCount cơ bản với MapReduce", "url": "/posts/hadoop-mapreduce-and-wordcount-project/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop MapReduce, MapReduce", "date": "2021-08-03 16:00:00 +0700", "snippet": "MapReduce là một kỹ thuật xử lý và là một mô hình lập trình cho tính toán phân tán để triển khai và xử lý dữ liệu lớn. Hadoop MapReduce là một khung xử lý dữ liệu của Hadoop xây dựng dựa trên ý tưởng của MapReduce, bây giờ khi nói về MapReduce là chúng ta sẽ nghĩ ngay tới Hadoop MapReduce, nên tr..." }, { "title": "Docker cơ bản và thực hành", "url": "/posts/Docker/", "categories": "Blogging", "tags": "Docker", "date": "2021-07-28 20:52:00 +0700", "snippet": "Bài viết được tham khảo và ghi chú lại từ series thực hành docker tại Viblo.Link series tại đây: Thực hành Dokcer từ căn bảnI. Cài đặt docker và một số khái niệm cơ bảnPhần cài đặt có thể tham khảo trên docDOCKER LÀ GÌCác điểm lợi từ khi ứng dụng được đóng gói (containerized app) Build 1 lần dùn..." }, { "title": "Tìm hiểu về Apache Nifi", "url": "/posts/Apache-Nifi/", "categories": "Blogging", "tags": "Big data, Data Ingestion, Apache Nifi", "date": "2021-07-12 20:52:00 +0700", "snippet": "Apache nifi được sử dụng để tự động hóa và kiểm soát các luồng dữ liệu giữa các hệ thống. Nó cung cấp cho chúng ta một giao diện trên nền web mà có thể thu thập, xử lý, phân tích dữ liệu.NiFi được biết đến với khả năng xây dựng luồng chuyển dữ liệu tự động giữa các hệ thống. Đặc biết là hỗ trợ rấ..." }, { "title": "Kafka In Depth", "url": "/posts/Kafka-In-Depth/", "categories": "Blogging", "tags": "Bigdata, Data Ingestion, Apache Kafka", "date": "2021-07-08 20:52:00 +0700", "snippet": "Trong quá trình làm bài tập lớn môn lưu trữ và xử lý dữ liệu lớn ở trường mình có biết đến kafka và sử dụng cho project của mình. Tuy nhiên lúc đó mình mới chỉ biết đơn giản nó là một message queue để đổ dữ liệu vào giúp việc đọc ghi từ nguồn vào đích không phụ thuộc lẫn nhau(loosely couple). Vậy..." }, { "title": "Các câu lệnh thao tác với file và thư mục trên HDFS", "url": "/posts/hdfs-commands/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS", "date": "2021-07-06 16:00:00 +0700", "snippet": "Các câu lệnh trên HDFS nhìn chung khá là giống với các câu lệnh trên Linux kể cả về chức năng lẫn tên của chúng, nếu bạn nào đã quen với Linux/Ubuntu rồi thì chắc cũng không cần phải học gì nhiều đâu, cứ thế áp dụng vào thôi.helpMuốn xin sự trợ giúp về common line trong HDFS:hdfs dfs -helphdfs df..." }, { "title": "HDFS", "url": "/posts/hdfs-introduction/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS", "date": "2021-07-04 16:00:00 +0700", "snippet": "Hadoop Distributed File System (HDFS) là hệ thống lưu trữ phân tán được thiết kế để chạy trên các phần cứng thông dụng. HDFS có khả năng chịu lỗi cao được triển khai sử dụng các phần cứng giá rẻ. HDFS cung cấp khả năng truy cập thông lượng cao vào dữ liệu ứng dụng vì thế nó rất phù hợp với ứng dụ..." }, { "title": "Hadoop Ecosystem", "url": "/posts/hadoopo-ecosystem/", "categories": "Hadoop & Spark, Hadoop", "tags": "Ubuntu, Hadoop Ecosystem, Bigdata, Java, HDFS, Pig, Kafka, Zookeeper, Hive, HBase, Sqoop", "date": "2021-07-02 08:00:00 +0700", "snippet": "Hệ sinh thái Apache Hadoop đề cập đến các thành phần khác nhau của thư viện phần mềm Apache Hadoop; nó bao gồm các dự án mã nguồn mở cũng như một loạt các công cụ bổ sung hoàn chỉnh khác. Một số công cụ nổi tiếng nhất của hệ sinh thái Hadoop bao gồm HDFS, Hive, Pig, YARN, MapReduce, Spark, HBase,..." }, { "title": "Cài đặt và triển khai Hadoop single node", "url": "/posts/install-and-deploy-hadoop-single-node/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop Yarn", "date": "2021-07-01 16:00:00 +0700", "snippet": "Mỗi ngành công nghiệp lớn đang triển khai Apache Hadoop là khung tiêu chuẩn để xử lý và lưu trữ dữ liệu lớn. Hadoop được thiết kế để được triển khai trên một mạng lưới hàng trăm hoặc thậm chí hàng ngàn máy chủ chuyên dụng. Tất cả các máy này làm việc cùng nhau để đối phó với khối lượng lớn và nhi..." }, { "title": "Giới thiệu tổng quan Hadoop", "url": "/posts/hadoop-introduction/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop Yarn", "date": "2021-06-29 20:52:00 +0700", "snippet": "Hadoop là framework dựa trên 1 giải pháp tới từ Google để lưu trữ và xử lý dữ liệu lớn. Hadoop sử dụng giải thuật MapReduce xử lý song song các dữ liệu đầu vào. Tóm lại, Hadoop được sử dụng để phát triển các ứng dụng có thể thực hiện phân tích thống kê hoàn chỉnh trên dữ liệu số lượng lớn.Xem thê..." }, { "title": "Mô hình lập trình MapReduce cho Bigdata", "url": "/posts/mapreduce-programming-model/", "categories": "Hadoop & Spark, Hadoop", "tags": "Ubuntu, mapreduce, Bigdata, Java", "date": "2021-06-24 08:00:00 +0700", "snippet": "MapReduce là một kỹ thuật xử lý và là một mô hình lập trình cho tính toán phân tán để triển khai và xử lý dữ liệu lớn. MapReduce chứa 2 tác vụ quan trọng là map và reduce. WordCount là một ví dụ điển hình cho MapReduce mà sẽ được mình minh họa trong bài viết nàyTại sao Mapreduce lại ra đời?Như ph..." }, { "title": "Redis 101 (Part I)", "url": "/posts/redis-101-(part-1)/", "categories": "Blogging", "tags": "NoSQL, Bigdata, Ubuntu, Redis", "date": "2021-06-10 20:52:00 +0700", "snippet": "Tổng quanTrong thời gian học môn Lưu trữ và xử lý dữ liệu lớn ở trường, mình có được nghe qua về redis. Đây là một cơ sở dữ liệu dạng NoSQL. Khác với các cơ sở dữ liệu khác thì đây là một dạng lưu trữ in-memory với cách lưu trữ là key-value. Ngoài tác dụng lưu trữ dữ liệu redis còn dùng để cache,..." } ]
