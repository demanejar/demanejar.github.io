---
title: Spark RDD
author: trannguyenhan
date: 2021-06-11 20:52:00 +0700
categories: [Spark]
tags: [Spark, Apache Spark, Bigdata, RDD]
math: true
mermaid: true
---

## Resilient Distributed Datasets (RDDs)
Resilient Distributed Datasets (RDD) là một cấu trúc dữ liệu cơ bản của Spark. Nó là một tập hợp bất biến phân tán của một đối tượng có thể hoạt động song song. 

Có hai cách để tạo RDDs:
-   Tạo từ một tập hợp dữ liệu có sẵn trong ngôn ngữ sử dụng như Java, Python, Scala.
-   Lấy từ dataset hệ thống lưu trữ bên ngoài như HDFS, Hbase hoặc các cơ sở dữ liệu quan hệ.

Ví dụ tạo RDD từ 1 List với Java: 
```java
List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
JavaRDD<Integer> distData = sc.parallelize(data);
```

Ví dụ tạo RDD từ 1 file text: 
```java
JavaRDD<String> distFile = sc.textFile("data.txt");
```
## RDD Operations
RDD hỗ trợ 2 loại operations: 
- _transformations_: tạo tập dữ liệu mới từ tập dữ liệu hiện có 
- _actions_: trả về giá trị cho chương trình điều khiển sau khi thực hiện tính toán trên tập dữ liệu

Tất cả `transformations` trong Spark đều lazy, đây cũng là một tính chất quan trọng quyết định tới hiệu năng của Spark và đi phỏng vấn hay thi câu này khá được chú ý tới. Các `transformations` không tính toán kết quả ngay lập tức, thay vào đó nó nhớ phép biến đổi được áp dụng cho tập dữ liệu. Transformations chỉ thực hiện tính toán khi `actions` yêu cầu một kết quả trả về.

### Transformations

| Transformation | Ý nghĩa |
| --- | --- |
| **map**(_func_) | Trả về một tập dữ liệu mới bằng cách chuyển đổi mỗi phần tử cũ sang từng phần tử mới thông qua _func_|
| **filter**(_func_) | Trả về một tập dữ liệu mới nếu các phần tử trong tập dữ liệu được chọn có _func_ trả về giá trị true |
| **flatMap**(_func_) | Giống mới map, tuy nhiên map là chuyển đổi 1-1 còn với flatMap thì có thể là chuyển đổi 1-N, 1-1, 1-0 |
| **union**(_otherDataset_) | Trả về tập dữ liệu mới là hợp của 2 tập dữ liệu |
| **intersection**(_otherDataset_) | Trả về tập dữ liệu mới là hợp của 2 tập dữ liệu|
| **distinct**(\[_numPartitions_\])) | Trả về tập dữ liệu mới với các phần tử riêng biệt|
| **groupByKey**(\[_numPartitions_\]) | Khi tập dữ liệu ban đầu có dạng cặp (K,V), trả về tập dữ liệu khác có dạng là 1 cặp (K, Iterable<V>). Hiểu đơn giản là groupByKey nhóm các cặp có cùng Key lại với nhau|
| **reduceByKey**(_func_, \[_numPartitions_\]) | Tập dữ liệu ban đầu có dạng là 1 cặp (K,V) và kết quả cho ra là 1 cặp (K,V) khác, trong đó giá trị V được tổng hợp từ hàm _func_. **Chúng ta hiểu đơn giản** reduceByKey giống so mới groupByKey, tuy nhiên groupByKey chỉ giúp nhóm các Value lại trong khi đó reduceByKey có thêm 1 hàm _func_ để thực hiện phép toán tổng hợp các Value đó và trả về kết quả Value duy nhất.|
| **sortByKey**(\[_ascending_\], \[_numPartitions_\]) | Tập dữ liệu ban đầu có dạng 1 cặp (K,V), trả về 1 cặp (K,V) khác với các K đã được sắp xếp tăng dần hoặc giảm dần|

Còn 1 số các _Transformations_ khác mà do chúng không phổ biến và không được sử dụng thường xuyên nên mình không đề cập ở đây. Nếu bạn muốn tìm hiểu thêm thì đọc tại [Spark docs transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)

### Actions

| Actions | Ý nghĩa |
| --- | --- |
| **reduce**(_func_) | Tổng hợp các phần tử sử dụng hàm _func_ (nhận vào 2 tham số và trả về một)|
| **collect**() | Trả về tất cả các dữ liệu dưới dạng 1 mảng |
| **count**() | Trả về sống lượng phần tử của tập dữ liệu |
| **first**() | Trả về phần tử đầu tiên |
| **take**(n) | Trả về 1 mảng với n phần tử đầu tiên trong mảng đó|
| **saveAsTextFile**(_path_) | Ghi dữ liệu vào file text|
| **foreach**(_func_) | Duyệt từng phần tử của tập hợp|

Còn 1 số các _Actions_ khác mà do chúng không phổ biến và không được sử dụng thường xuyên nên mình không đề cập ở đây. Nếu bạn muốn tìm hiểu thêm thì đọc tại [Spark docs Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)

## Ví dụ một số Operations
  
Mở Spark shell trên terminal bằng lệnh: 
```bash
spark-shell
```

### map 
Các phần tử của tập dữ liệu ban đầu được nhân đôi và gán vào 1 biến khác: 

![](https://i.pinimg.com/564x/c6/f1/8e/c6f18e6241cc662fa817c78a1fde5f6d.jpg)

### filter 
Lọc ra các phần tử chia hết cho 5 của tập dữ liệu ban đầu và gán vào 1 biến khác: 

![](https://i.pinimg.com/originals/22/25/50/222550f477cf2fb4bec67989faa2f1f5.jpg)

### groupByKey
Nhóm các phần tử có chung K lại với nhau: 

![](https://i.pinimg.com/564x/0d/78/61/0d78610099b2f727da5ddbf9d2f39e1c.jpg)

### reduceByKey
Tổng hợp lại số lượng của các phần tử có Key trùng nhau: 

![](https://i.pinimg.com/564x/b1/64/f4/b164f4f6bc8810a842de8ba2541f1e09.jpg)


Tham khảo: [https://laptrinh.vn/](https://laptrinh.vn/books/apache-spark/page/apache-spark-rdd), [https://spark.apache.org/](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)
