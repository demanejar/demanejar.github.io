---
title: Spark RDD
author: trannguyenhan
date: 2021-06-11 20:52:00 +0700
categories: [Apache, Spark]
tags: [Spark, Apache Spark, Bigdata, RDD]
math: true
mermaid: true
image:
  src: https://i.pinimg.com/originals/ab/83/c3/ab83c36fb99663705ad01c49014f3e61.jpg
---

## Resilient Distributed Datasets (RDDs)
Resilient Distributed Datasets (RDD) là một cấu trúc dữ liệu cơ bản của Spark. Nó là một tập hợp bất biến phân tán của một đối tượng có thể hoạt động song song. 

Có hai cách để tạo RDDs:
-   Tạo từ một tập hợp dữ liệu có sẵn trong ngôn ngữ sử dụng như Java, Python, Scala.
-   Lấy từ dataset hệ thống lưu trữ bên ngoài như HDFS, Hbase hoặc các cơ sở dữ liệu quan hệ.

Ví dụ tạo RDD từ 1 List với Java: 
```java
List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
JavaRDD<Integer> distData = sc.parallelize(data);
```

Ví dụ tạo RDD từ 1 file text: 
```java
JavaRDD<String> distFile = sc.textFile("data.txt");
```
## RDD Operations
RDD hỗ trợ 2 loại operations: 
- _transformations_: tạo tập dữ liệu mới từ tập dữ liệu hiện có 
- _actions_: trả về giá trị cho chương trình điều khiển sau khi thực hiện tính toán trên tập dữ liệu

Tất cả `transformations` trong Spark đều lazy, đây cũng là một tính chất quan trọng quyết định tới hiệu năng của Spark và đi phỏng vấn hay thi câu này khá được chú ý tới. Các `transformations` không tính toán kết quả ngay lập tức, thay vào đó nó nhớ phép biến đổi được áp dụng cho tập dữ liệu. Transformations chỉ thực hiện tính toán khi `actions` yêu cầu một kết quả trả về.

### Transformations

| Transformation | Ý nghĩa |
| --- | --- |
| **map**(_func_) | Trả về một tập dữ liệu mới bằng cách chuyển đổi mỗi phần tử cũ sang từng phần tử mới thông qua _func_|
| **filter**(_func_) | Trả về một tập dữ liệu mới nếu các phần tử trong tập dữ liệu được chọn có _func_ trả về giá trị true |
| **flatMap**(_func_) | Giống mới map, tuy nhiên map là chuyển đổi 1-1 còn với flatMap thì có thể là chuyển đổi 1-N, 1-1, 1-0 |
| **union**(_otherDataset_) | Trả về tập dữ liệu mới là hợp của 2 tập dữ liệu |
| **intersection**(_otherDataset_) | Trả về tập dữ liệu mới là hợp của 2 tập dữ liệu|
| **distinct**(\[_numPartitions_\])) | Trả về tập dữ liệu mới với các phần tử riêng biệt|
| **groupByKey**(\[_numPartitions_\]) | Khi tập dữ liệu ban đầu có dạng cặp (K,V), trả về tập dữ liệu khác có dạng là 1 cặp (K, Iterable<V>). Hiểu đơn giản là *groupByKey* giúp nhóm các cặp có cùng Key lại với nhau|
| **reduceByKey**(_func_, \[_numPartitions_\]) | Tập dữ liệu ban đầu có dạng là 1 cặp (K,V) và kết quả cho ra là 1 cặp (K,V) khác, trong đó giá trị V được tổng hợp từ hàm _func_. Chúng ta hiểu đơn giản, _reduceByKey_ giống so mới _groupByKey_, tuy nhiên _groupByKey_ chỉ giúp nhóm các Value lại trong khi đó _reduceByKey_ có thêm 1 hàm _func_ để thực hiện phép toán tổng hợp các Value đó và trả về kết quả Value duy nhất|
| **sortByKey**(\[_ascending_\], \[_numPartitions_\]) | Tập dữ liệu ban đầu có dạng 1 cặp (K,V), trả về 1 cặp (K,V) khác với các K đã được sắp xếp tăng dần hoặc giảm dần|

Còn 1 số các _Transformations_ khác mà do chúng không phổ biến và không được sử dụng thường xuyên nên mình không đề cập ở đây. Nếu bạn muốn tìm hiểu thêm thì đọc tại [Spark docs transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)

### Actions

| Actions | Ý nghĩa |
| --- | --- |
| **reduce**(_func_) | Tổng hợp các phần tử sử dụng hàm _func_ (nhận vào 2 tham số và trả về một)|
| **collect**() | Trả về tất cả các dữ liệu dưới dạng 1 mảng |
| **count**() | Trả về sống lượng phần tử của tập dữ liệu |
| **first**() | Trả về phần tử đầu tiên |
| **take**(n) | Trả về 1 mảng với n phần tử đầu tiên trong mảng đó|
| **saveAsTextFile**(_path_) | Ghi dữ liệu vào file text|
| **foreach**(_func_) | Duyệt từng phần tử của tập hợp|

Còn 1 số các _Actions_ khác mà do chúng không phổ biến và không được sử dụng thường xuyên nên mình không đề cập ở đây. Nếu bạn muốn tìm hiểu thêm thì đọc tại [Spark docs Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)

## Ví dụ một số Operations
Tạo một maven project bằng Java và thêm vào các phụ thuộc sau: 
```
<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
<dependency>
	<groupId>org.apache.spark</groupId>
	<artifactId>spark-core_2.12</artifactId>
	<version>3.1.1</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
<dependency>
	<groupId>org.apache.hadoop</groupId>
	<artifactId>hadoop-client</artifactId>
	<version>3.3.0</version>
</dependency>
```
### map 
Ta có một tập dữ liệu có giá trị các phần tử là 10,20,30. Bây giờ chúng ta sẽ dùng `map` để gấp đôi từng phần tử trong tập dữ liệu ban đầu.
```java
package demo;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

public class Main {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("Demo")
				.setMaster("local[2]");
		
		try (JavaSparkContext sc = new JavaSparkContext(conf)) {
			List<Integer> list = Arrays.asList(10,20,30);
			JavaRDD<Integer> data = sc.parallelize(list);
			
			data = data.map(new Function<Integer, Integer>() {
				private static final long serialVersionUID = 1L;

				@Override
				public Integer call(Integer v1) throws Exception {
					return v1 * 2;
				}
				
			});
			
			data.collect().forEach(v -> System.out.println(v));
		}
		
	}
}
```

Kết quả cho ra lần lượt là: 
```bash 
> 10
> 20
> 30
```

### filter 
Ta có tập dữ liệu ban đầu là 10, 11, 12, 13, 14, 15. Sau đó sử dụng `flatMap` để lọc ra các phần tử chia hết cho 5.
```java
package filter;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

public class Main {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("Demo")
				.setMaster("local[2]");
	
		try (JavaSparkContext sc = new JavaSparkContext(conf)) {
			List<Integer> list = Arrays.asList(10,11,12,13,14,15);
			JavaRDD<Integer> data = sc.parallelize(list);
			
			data = data.filter(new Function<Integer, Boolean>() {
				private static final long serialVersionUID = 1L;

				@Override
				public Boolean call(Integer v1) throws Exception {
					if(v1 % 5 == 0) return true;
					return false;
				}
			});
			
			data.collect().forEach(v -> System.out.println(v));
		}
	}
}
```

Kết quả cho ra lần lượt là: 
```bash
> 10
> 15
```

### groupByKey
Ta có 1 tập dữ liệu ban đầu dạng (K,V). `groupByKey` sẽ giúp nhóm khác phần tử cùng key lại với nhau.
```java
package groupbykey;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class Main {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("Demo")
				.setMaster("local[2]");
	
		try (JavaSparkContext sc = new JavaSparkContext(conf)) {
			List<Tuple2<String, Integer>> list = Arrays.asList(
					new Tuple2<String, Integer>("C", 3), 
					new Tuple2<String, Integer>("A", 1), 
					new Tuple2<String, Integer>("B", 4), 
					new Tuple2<String, Integer>("A", 2), 
					new Tuple2<String, Integer>("B", 5));
			
			JavaPairRDD<String, Integer> data = sc.parallelizePairs(list);
			data.groupByKey().collect().forEach(s -> System.out.println(s));
			
		}
	}
}
```

Kết quả sẽ là: 
```bash
> (B,[4, 5])
> (A,[1, 2])
> (C,[3])
```

### reduceByKey
`reduceByKey` ngoài việc nhóm các phần tử cùng key lại với nhau nó còn giúp thực hiện tính toán trên các Value cùng Key đó.
```java
package reducebykey;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;

import scala.Tuple2;

public class Main {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("Demo")
				.setMaster("local[2]");
	
		try (JavaSparkContext sc = new JavaSparkContext(conf)) {
			List<Tuple2<String, Integer>> list = Arrays.asList(
					new Tuple2<String, Integer>("C", 3), 
					new Tuple2<String, Integer>("A", 1), 
					new Tuple2<String, Integer>("B", 4), 
					new Tuple2<String, Integer>("A", 2), 
					new Tuple2<String, Integer>("B", 5));
			
			JavaPairRDD<String, Integer> data = sc.parallelizePairs(list);
			data = data.reduceByKey(new Function2<Integer, Integer, Integer>() {
				private static final long serialVersionUID = 1L;

				@Override
				public Integer call(Integer v1, Integer v2) throws Exception {
					return v1 + v2;
				}
			});
			
			data.collect().forEach(v -> System.out.println(v));
			
		}
	}
}
```

Kết quả là: 
```bash
> (B,9)
> (A,3)
> (C,3)
```

Xem toàn bộ mã nguồn của ví dụ tại: [https://github.com/demanejar/spark-rdd](https://github.com/demanejar/spark-rdd)

Tham khảo: [https://laptrinh.vn/](https://laptrinh.vn/books/apache-spark/page/apache-spark-rdd), [https://spark.apache.org/](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)
